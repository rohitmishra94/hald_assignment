# Fine-Grained Plankton Identification System

A high-precision 2-stage cascade pipeline for plankton species identification from microscopy images, addressing the challenges of high intra-class variance and inter-class similarity in fine-grained biological classification.

## üéØ Overview

This project implements a production-ready plankton identification system using a **2-stage cascade architecture**:

1. **Stage 1 (YOLO Detection)**: Detect ALL plankton objects with high recall
2. **Stage 2 (ArcFace Classification)**: Identify species using deep metric learning

### Key Challenges Addressed
- **High intra-class variance**: Same species looks different due to rotation, lighting, morphology
- **High inter-class similarity**: Different species look nearly identical
- **Class imbalance**: Rare species with few samples
- **Fine-grained features**: Subtle differences between species

## üìä Performance

### Stage 1: YOLO Detection
- **Model**: YOLOv10-Large (single "plankton" super-class)
- **mAP@0.5**: 91.6%
- **Recall**: 85.9%
- **Resolution**: 1920√ó1080 (full native resolution)

### Stage 2: ArcFace Identification
- **Model**: ResNet50 + Sub-Center ArcFace (K=5)
- **Top-1 Accuracy**: 94.83%
- **Top-5 Accuracy**: 98.28%
- **F1-Macro**: 0.8391 (all classes treated equally)
- **F1-Weighted**: 0.9484 (weighted by class frequency)
- **Classes**: 39 plankton species

### End-to-End Pipeline
- **Combined Accuracy**: ~85-90% (high confidence predictions only)
- **Speed**: ~50-100 FPS (depends on object density)
- **Confidence Thresholds**: YOLO=0.15, ArcFace=0.6

## üöÄ Quick Start

### 1. Installation

```bash
# Clone repository
git clone https://github.com/rohitmishra94/hald_assignment.git
cd hald_assignment

# Install dependencies
pip install -r requirements.txt

# Install YOLOv10
pip install ultralytics
```

### 2. Dataset Preparation

```bash
# Create YOLO super-class dataset (single "plankton" class)
python create_superclass_yolo_dataset.py

# Create ArcFace dataset (cropped 128√ó128 objects by species)
python prepare_arcface_dataset.py
```

**Output**:
- `yolo_superclass_dataset/` - YOLO training data
- `arcface_dataset/` - ArcFace training data (train/test splits)

### 3. Training

#### Stage 1: Train YOLO Detector
```bash
bash train_yolo_cascade.sh
```

**Configuration**:
- Model: YOLOv10-Large
- Resolution: 1920√ó1080
- Epochs: 100
- Batch size: 2
- Confidence: 0.15 (high recall)

#### Stage 2: Train ArcFace Identifier
```bash
cd evaluation
python train_arc.py \
    --dataset ../arcface_dataset \
    --backbone resnet50 \
    --epochs 100 \
    --batch-size 32 \
    --lr 0.0001
```

**Configuration**:
- Backbone: ResNet50
- Optimizer: AdamW (lr=0.0001)
- Scheduler: CosineAnnealingLR
- Sub-Center ArcFace: K=5, s=30, m=0.35
- Augmentation: Rotation, affine, color jitter, random erasing

**Training outputs** (saved to `arcface_models/`):
- `best_model.pth` - Best model (based on F1-Macro)
- `classification_report.txt` - Per-class metrics
- `confusion_matrix.png` - Species confusion heatmap
- `training_history.png` - Loss/accuracy/F1 curves

### 4. Generate Class Prototypes

```bash
python generate_prototypes.py \
    --model arcface_models/best_model.pth \
    --dataset arcface_dataset \
    --backbone resnet50 \
    --output arcface_models/class_prototypes.pth
```

**Output**:
- `class_prototypes.pth` - Averaged embeddings per class
- `embedding_visualization_umap.png` - UMAP visualization
- `prototype_statistics.txt` - Inter-class similarity analysis

### 5. Run Inference

#### Single Image
```bash
python cascade_inference.py \
    --image path/to/image.jpg \
    --yolo yolo_cascade_training/yolo_superclass_plankton/weights/best.pt \
    --arcface arcface_models/best_model.pth \
    --prototypes arcface_models/class_prototypes.pth \
    --yolo-conf 0.15 \
    --arcface-conf 0.6
```

#### Batch Processing
```bash
python cascade_inference.py \
    --batch path/to/images/ \
    --output cascade_results/ \
    --yolo-conf 0.15 \
    --arcface-conf 0.6
```

## üèóÔ∏è System Architecture

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                     STAGE 1: DETECTION                       ‚îÇ
‚îÇ                                                              ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê        ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê                    ‚îÇ
‚îÇ  ‚îÇ   Image    ‚îÇ   ‚Üí    ‚îÇ   YOLOv10    ‚îÇ   ‚Üí  Bounding Boxes‚îÇ
‚îÇ  ‚îÇ 1920√ó1080  ‚îÇ        ‚îÇ (Super-Class)‚îÇ                    ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò        ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò                     ‚îÇ
‚îÇ                                                              ‚îÇ
‚îÇ  Goal: Detect ALL plankton objects (High Recall)            ‚îÇ
‚îÇ  Confidence: 0.15 (low threshold to catch everything)       ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                              ‚Üì
                    Cropped Object Images
                              ‚Üì
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                   STAGE 2: IDENTIFICATION                    ‚îÇ
‚îÇ                                                              ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê        ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê                    ‚îÇ
‚îÇ  ‚îÇ  Cropped   ‚îÇ   ‚Üí    ‚îÇ   ResNet50   ‚îÇ   ‚Üí  512-dim      ‚îÇ
‚îÇ  ‚îÇ 128√ó128    ‚îÇ        ‚îÇ   Backbone   ‚îÇ      Embedding     ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò        ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò                     ‚îÇ
‚îÇ                              ‚Üì                               ‚îÇ
‚îÇ                     L2 Normalization                         ‚îÇ
‚îÇ                              ‚Üì                               ‚îÇ
‚îÇ                    Cosine Similarity                         ‚îÇ
‚îÇ                              ‚Üì                               ‚îÇ
‚îÇ                     Class Prototypes                         ‚îÇ
‚îÇ                              ‚Üì                               ‚îÇ
‚îÇ                   Species ID + Confidence                    ‚îÇ
‚îÇ                                                              ‚îÇ
‚îÇ  Goal: Identify species using metric learning               ‚îÇ
‚îÇ  Sub-Center ArcFace (K=5) for intra-class variance          ‚îÇ
‚îÇ  Confidence: 0.6 (only trust high similarity predictions)   ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

## üìÅ Project Structure

```
hald_assignment/
‚îú‚îÄ‚îÄ README.md                           # This file
‚îú‚îÄ‚îÄ CASCADE_PIPELINE_README.md          # Detailed technical documentation
‚îÇ
‚îú‚îÄ‚îÄ data_audit/                         # Data analysis and visualization
‚îÇ   ‚îú‚îÄ‚îÄ data_analysis.ipynb             # Class distribution, bbox analysis
‚îÇ   ‚îú‚îÄ‚îÄ intra_class_analysis.ipynb      # Intra-class variance study
‚îÇ   ‚îî‚îÄ‚îÄ data_audit/                     # Generated visualizations
‚îÇ
‚îú‚îÄ‚îÄ evaluation/                         # Training and model code
‚îÇ   ‚îú‚îÄ‚îÄ model_arc.py                    # ArcFace models (ResNet + ArcFace Head)
‚îÇ   ‚îú‚îÄ‚îÄ train_arc.py                    # ArcFace training script
‚îÇ   ‚îî‚îÄ‚îÄ inference_arc.py                # ArcFace inference utilities
‚îÇ
‚îú‚îÄ‚îÄ workspace/some_exp/genus/hald_assignment/StudyCase/
‚îÇ   ‚îú‚îÄ‚îÄ _annotations.coco.json          # Original COCO annotations
‚îÇ   ‚îî‚îÄ‚îÄ images/                         # Original 1920√ó1080 images
‚îÇ
‚îú‚îÄ‚îÄ yolo_superclass_dataset/            # Generated YOLO dataset
‚îÇ   ‚îú‚îÄ‚îÄ images/train/                   # Training images
‚îÇ   ‚îú‚îÄ‚îÄ images/val/                     # Validation images
‚îÇ   ‚îú‚îÄ‚îÄ labels/train/                   # YOLO format labels
‚îÇ   ‚îú‚îÄ‚îÄ labels/val/
‚îÇ   ‚îî‚îÄ‚îÄ dataset.yaml                    # YOLO config
‚îÇ
‚îú‚îÄ‚îÄ arcface_dataset/                    # Generated ArcFace dataset
‚îÇ   ‚îú‚îÄ‚îÄ train/                          # class1/, class2/, ... (cropped 128√ó128)
‚îÇ   ‚îú‚îÄ‚îÄ test/                           # Test set (20%)
‚îÇ   ‚îú‚îÄ‚îÄ dataset_info.json               # Dataset statistics
‚îÇ   ‚îî‚îÄ‚îÄ class_mapping.json              # Class name to index mapping
‚îÇ
‚îú‚îÄ‚îÄ yolo_cascade_training/              # YOLO training outputs
‚îÇ   ‚îî‚îÄ‚îÄ yolo_superclass_plankton/
‚îÇ       ‚îî‚îÄ‚îÄ weights/best.pt             # Best YOLO model
‚îÇ
‚îú‚îÄ‚îÄ arcface_models/                     # ArcFace training outputs
‚îÇ   ‚îú‚îÄ‚îÄ best_model.pth                  # Best ArcFace model (F1-Macro)
‚îÇ   ‚îú‚îÄ‚îÄ class_prototypes.pth            # Class prototypes for inference
‚îÇ   ‚îú‚îÄ‚îÄ classification_report.txt       # Per-class metrics
‚îÇ   ‚îú‚îÄ‚îÄ confusion_matrix.png            # Confusion heatmap
‚îÇ   ‚îú‚îÄ‚îÄ training_history.png            # Training curves
‚îÇ   ‚îî‚îÄ‚îÄ embedding_visualization_umap.png # Embedding space
‚îÇ
‚îú‚îÄ‚îÄ create_superclass_yolo_dataset.py   # COCO ‚Üí YOLO super-class
‚îú‚îÄ‚îÄ prepare_arcface_dataset.py          # COCO ‚Üí Cropped ArcFace dataset
‚îú‚îÄ‚îÄ generate_prototypes.py              # Generate class prototypes
‚îú‚îÄ‚îÄ cascade_inference.py                # Unified inference pipeline
‚îú‚îÄ‚îÄ train_yolo_cascade.sh               # YOLO training script
‚îî‚îÄ‚îÄ split_dataset.py                    # Dataset splitting utilities
```

## üî¨ Technical Details

### Stage 1: YOLO Detection

**Why Single Super-Class?**
- Simplifies detection (detect ANY plankton object)
- Optimizes for **high recall** (don't miss any objects)
- False positives acceptable (Stage 2 filters them out)

**Key Parameters**:
- Low confidence threshold (0.15) for high recall
- Full 1920√ó1080 resolution (no downsampling)
- YOLOv10-Large for better small object detection

### Stage 2: ArcFace Identification

**Why Sub-Center ArcFace?**
- Handles **intra-class variance** (K=5 sub-centers per class)
- Standard ArcFace would struggle with morphological variations
- Allows multiple "modes" per species

**Architecture**:
- **Backbone**: ResNet50 (pretrained on ImageNet)
- **Embedding**: 512-dimensional L2-normalized
- **Loss**: Sub-Center ArcFace (K=5, s=30, m=0.35)
- **Optimizer**: AdamW (better than SGD for metric learning)
- **Scheduler**: CosineAnnealingLR (smooth decay)

**Training Strategy**:
- Hard labels (no label smoothing) - maintains angular margins
- Aggressive augmentation (rotation, affine, color, erasing)
- Lower angular margin (m=0.35) for similar species
- Lower scale (s=30) for stable gradients

**Inference**:
- Discard ArcFace Head (no longer needed)
- Extract L2-normalized embeddings
- Match against class prototypes using cosine similarity
- Confidence threshold (0.6) for final predictions

## üìà Evaluation Metrics

### Why Multiple Metrics?

**Accuracy alone is misleading** for imbalanced datasets:
- A model could get 90% accuracy by predicting only common classes
- Rare species would be ignored

### Metrics Used

1. **Top-1 Accuracy**: % of exactly correct predictions
   - Good for overall performance
   - Can be misleading with class imbalance

2. **Top-5 Accuracy**: % where correct class is in top 5
   - Shows if model is "close" when wrong
   - High Top-5 (>95%) means model understands features

3. **F1-Macro**: Average F1 across all classes (unweighted)
   - **Best metric for imbalanced data**
   - Treats rare and common classes equally
   - Used for model selection

4. **F1-Weighted**: F1 weighted by class frequency
   - Shows performance on common classes
   - Usually higher than F1-Macro

5. **Precision**: % of predictions that are correct
   - Important for production (avoid misidentification)

6. **Recall**: % of instances detected
   - Important for rare species (don't miss them)

### Interpreting Results

**Example from this project**:
```
Accuracy:     94.83%   ‚úÖ Excellent overall
Top-5 Acc:    98.28%   ‚úÖ Model is "close" when wrong
F1-Macro:     0.8391   ‚ö†Ô∏è Rare classes struggling
F1-Weighted:  0.9484   ‚úÖ Common classes excellent
```

**Interpretation**:
- Overall performance is excellent
- Common species identified nearly perfectly
- Rare species (1-10 samples) have ~60-80% accuracy
- Gap between F1-Macro and F1-Weighted indicates imbalance

## üéì Key Improvements from Baseline

### Original Configuration
- SGD optimizer
- ResNet18 backbone
- Limited augmentation
- Accuracy-based model selection
- **Result**: 78% accuracy

### Improved Configuration
- ‚úÖ AdamW optimizer (better convergence)
- ‚úÖ ResNet50 backbone (25M vs 11M parameters)
- ‚úÖ Strong augmentation (rotation, affine, erasing)
- ‚úÖ F1-Macro model selection (handles imbalance)
- ‚úÖ Optimized ArcFace parameters (K=5, s=30, m=0.35)
- ‚úÖ CosineAnnealingLR (smooth decay)
- **Result**: 94.83% accuracy, 0.8391 F1-Macro

**Improvement**: +16.83% accuracy, significantly better rare class performance

## üîß Hyperparameter Tuning Guide

### If Accuracy is Low (<85%)
- Increase augmentation strength
- Train longer (100+ epochs)
- Use larger backbone (ResNet50 ‚Üí ResNet101)
- Lower learning rate (0.0001 ‚Üí 0.00005)

### If Rare Classes Struggle (F1-Macro << Accuracy)
- Increase sub-centers (K=5 ‚Üí K=10)
- Augment rare classes more aggressively
- Use class-weighted loss
- Reduce angular margin (m=0.35 ‚Üí m=0.25)

### If Similar Species Confused (Low Precision)
- Increase angular margin (m=0.35 ‚Üí m=0.5)
- Add more training data for confused pairs
- Use hard negative mining
- Increase embedding dimension (512 ‚Üí 1024)

### If Training is Unstable
- Lower learning rate
- Use gradient clipping
- Increase batch size
- Lower scale parameter (s=30 ‚Üí s=20)

## üìö References

### Papers
- **ArcFace**: [Additive Angular Margin Loss for Deep Face Recognition](https://arxiv.org/abs/1801.07698)
- **Sub-Center ArcFace**: [Boosting Face Recognition by Large-scale Noisy Web Faces](https://www.ecva.net/papers/eccv_2020/papers_ECCV/papers/123560715.pdf)
- **YOLOv10**: [Real-Time End-to-End Object Detection](https://arxiv.org/abs/2405.14458)

### Tools
- [Ultralytics YOLOv10](https://github.com/THU-MIG/yolov10)
- [PyTorch](https://pytorch.org/)
- [Albumentations](https://albumentations.ai/)

## üë®‚Äçüíª Author

**Rohit Mishra**
- Email: rohit225151@rediffmail.com
- GitHub: [@rohitmishra94](https://github.com/rohitmishra94)

## üìÑ License

MIT License

## üôè Acknowledgments

- Claude Code for development assistance
- Hadl assignment case study for problem definition
- ArcFace and YOLOv10 authors for excellent papers

---

**Generated with Claude Code** ü§ñ
